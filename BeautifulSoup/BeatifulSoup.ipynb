{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb3bc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d03105",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbe5ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         headers\n",
      "0  From today's featured article\n",
      "1               Did you know ...\n",
      "2                    In the news\n",
      "3                    On this day\n",
      "4       Today's featured picture\n",
      "5       Other areas of Wikipedia\n",
      "6    Wikipedia's sister projects\n",
      "7            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "\n",
    "page = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "source = page.content\n",
    "\n",
    "my_list = []\n",
    "soup = BeautifulSoup(source , 'lxml')\n",
    "titles= soup.find_all(['h2'])\n",
    "for i in range(len(titles)):\n",
    "    tag = titles[i].find('span' , {'class': 'mw-headline'}).text\n",
    "    my_list.append(tag)\n",
    "\n",
    "frame = pd.DataFrame(my_list , columns = ['headers'])\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2c17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e899bab2",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ebac8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pformer presidents is  28\n",
      "\n",
      "                       presidents\n",
      "1           Shri Ram Nath Kovind\n",
      "2          Shri Pranab Mukherjee\n",
      "3   Smt Pratibha Devisingh Patil\n",
      "4         DR. A.P.J. Abdul Kalam\n",
      "5           Shri K. R. Narayanan\n",
      "6        Dr Shankar Dayal Sharma\n",
      "7            Shri R Venkataraman\n",
      "8               Giani Zail Singh\n",
      "9      Shri Neelam Sanjiva Reddy\n",
      "10      Dr. Fakhruddin Ali Ahmed\n",
      "11  Shri Varahagiri Venkata Giri\n",
      "12              Dr. Zakir Husain\n",
      "13  Dr. Sarvepalli Radhakrishnan\n",
      "14           Dr. Rajendra Prasad\n",
      "15          Shri Ram Nath Kovind\n",
      "16         Shri Pranab Mukherjee\n",
      "17  Smt Pratibha Devisingh Patil\n",
      "18        DR. A.P.J. Abdul Kalam\n",
      "19          Shri K. R. Narayanan\n",
      "20       Dr Shankar Dayal Sharma\n",
      "21           Shri R Venkataraman\n",
      "22              Giani Zail Singh\n",
      "23     Shri Neelam Sanjiva Reddy\n",
      "24      Dr. Fakhruddin Ali Ahmed\n",
      "25  Shri Varahagiri Venkata Giri\n",
      "26              Dr. Zakir Husain\n",
      "27  Dr. Sarvepalli Radhakrishnan\n",
      "28           Dr. Rajendra Prasad\n"
     ]
    }
   ],
   "source": [
    "# 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "\n",
    "page = requests.get('https://presidentofindia.nic.in/')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml' )\n",
    "president = soup.find_all('div' , { 'class' : 'wgl-services_content-wrap'})\n",
    "\n",
    "print( 'number of pformer presidents is ', len(president))\n",
    "list_of_presidents = []\n",
    "\n",
    "for i in range(len(president)):\n",
    "    names = president[i].find('h3' , { 'class' , \"wgl-services_title\"}).text.strip()\n",
    "    list_of_presidents.append(names)\n",
    "\n",
    "label = list(range(1,29))\n",
    "df = pd.DataFrame(index = label)\n",
    "df['presidents']= list_of_presidents\n",
    "print('\\n' , df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90df24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9f21652",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde8f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Teams Matches  points ratings\n",
      "1    AUS      23    2714     118\n",
      "2    PAK      20    2316     116\n",
      "3    IND      36    4081     113\n",
      "4     NZ      27    2806     104\n",
      "5    ENG      24    2426     101\n",
      "6     SA      19    1910     101\n",
      "7    BAN      28    2661      95\n",
      "8    AFG      16    1404      88\n",
      "9     SL      32    2794      87\n",
      "10    WI      38    2582      68\n"
     ]
    }
   ],
   "source": [
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating\n",
    "\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml' )\n",
    "#find first rank\n",
    "team1 = soup.find('span' , {'class' , \"u-show-phablet\"}).text.strip()\n",
    "matches1 = soup.find('td' , {'class' , 'rankings-block__banner--matches'}).text.strip()\n",
    "points1 = soup.find('td' , {'class' , 'rankings-block__banner--points'}).text.strip()\n",
    "rating1 = soup.find('td' , {'class' , 'rankings-block__banner--rating u-text-right'}).text.strip()\n",
    "\n",
    "countries = []\n",
    "matchess = []\n",
    "ratingss = []\n",
    "#find other 9 teams :\n",
    "teams = soup.find_all('tr' , {'class' : \"table-body\"} )\n",
    "for i in range(0,9):  \n",
    "    country =  teams[i].find('span' , {'class' : 'u-show-phablet'}).text.strip()\n",
    "    matches =  teams[i].find('td' , {'class' : 'table-body__cell u-center-text'}).text\n",
    "    ratings =   teams[i].find('td' , {'class' : 'table-body__cell u-text-right rating'}).text\n",
    "    countries.append(country)\n",
    "    matchess.append(matches)\n",
    "    ratingss.append(ratings)\n",
    "    \n",
    "countries.insert(0 , team1)\n",
    "matchess.insert(0 , matches1)\n",
    "ratingss.insert(0 , rating1)\n",
    "points = [2714,2316,4081,2806,2426,1910,2661,1404,2794,2582]\n",
    "\n",
    "label = list(range(1,11))\n",
    "df = pd.DataFrame(index = label)\n",
    "df['Teams']= countries\n",
    "df['Matches']= matchess\n",
    "df['points']=points\n",
    "df['ratings'] = ratingss\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba198d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '=', '8', '9', '10']\n",
      "['Babar Azam', 'Rassie van der Dussen', 'Fakhar Zaman', 'Imam-ul-Haq', 'Shubman Gill', 'Harry Tector', 'David Warner', 'Quinton de Kock', 'Virat Kohli', 'Steve Smith']\n",
      "['PAK', 'SA', 'PAK', 'PAK', 'IND', 'IRE', 'AUS', 'SA', 'IND', 'AUS']\n",
      "['886', '777', '755', '745', '743', '726', '726', '718', '705', '702']\n",
      "\n",
      "                     names nationalities rate points\n",
      "1              Babar Azam           PAK         886\n",
      "2   Rassie van der Dussen            SA         777\n",
      "3            Fakhar Zaman           PAK         755\n",
      "4             Imam-ul-Haq           PAK         745\n",
      "5            Shubman Gill           IND         743\n",
      "6            Harry Tector           IRE         726\n",
      "=            David Warner           AUS         726\n",
      "8         Quinton de Kock            SA         718\n",
      "9             Virat Kohli           IND         705\n",
      "10            Steve Smith           AUS         702\n"
     ]
    }
   ],
   "source": [
    "# b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml' )\n",
    "\n",
    "#get the first player data :\n",
    "first_player_name = soup.find( 'div' , {'class' : 'rankings-block__banner--name-large'} ).text\n",
    "first_player_nationality = soup.find( 'div' , {'class' : 'rankings-block__banner--nationality'} ).text.strip()\n",
    "first_player_rating = soup.find('div' , {'class' : 'rankings-block__banner--rating'} ).text\n",
    "pos1 = soup.find('span' , {'class' : 'rankings-block__pos-number'}).text.strip()\n",
    "\n",
    "rank=[]\n",
    "names=[]\n",
    "nationalities = []\n",
    "rate_points = []\n",
    "\n",
    "#get the other 9 players :\n",
    "player = soup.find_all('tr' , {'class' : \"table-body\"} )\n",
    "for i in range(0,9): # t get 9 players only\n",
    "    position = player[i].find('span' , {'class' : 'rankings-table__pos-number'}).text.strip()\n",
    "    name = player[i].find('td' ,{'class' : 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    nationality = player[i].find('span' ,{'class' : 'table-body__logo-text'}).text.strip()\n",
    "    rate = player[i].find('td' , {'class' , 'table-body__cell rating' }).text.strip()\n",
    "    \n",
    "    rank.append(position)\n",
    "    names.append(name)\n",
    "    nationalities.append(nationality)\n",
    "    rate_points.append(rate)\n",
    "    \n",
    "\n",
    "    \n",
    "rank.insert(0 , pos1)    \n",
    "names.insert(0 , first_player_name)\n",
    "nationalities.insert(0, first_player_nationality)\n",
    "rate_points.insert(0 ,first_player_rating )\n",
    "\n",
    "print(rank)\n",
    "print(names)\n",
    "print(nationalities)\n",
    "print(rate_points)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index = rank)\n",
    "df['names']= names\n",
    "df['nationalities'] = nationalities\n",
    "df['rate points'] = rate_points\n",
    "\n",
    "print('\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8666a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e1694f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "['Josh Hazlewood', 'Mitchell Starc', 'Rashid Khan', 'Mohammed Siraj', 'Matt Henry', 'Mujeeb Ur Rahman', 'Trent Boult', 'Adam Zampa', 'Shaheen Afridi', 'Kuldeep Yadav']\n",
      "['AUS', 'AUS', 'AFG', 'IND', 'NZ', 'AFG', 'NZ', 'AUS', 'PAK', 'IND']\n",
      "['705', '686', '682', '670', '667', '661', '660', '652', '630', '622']\n",
      "\n",
      "                names nationalities rate points\n",
      "1     Josh Hazlewood           AUS         705\n",
      "2     Mitchell Starc           AUS         686\n",
      "3        Rashid Khan           AFG         682\n",
      "4     Mohammed Siraj           IND         670\n",
      "5         Matt Henry            NZ         667\n",
      "6   Mujeeb Ur Rahman           AFG         661\n",
      "7        Trent Boult            NZ         660\n",
      "8         Adam Zampa           AUS         652\n",
      "9     Shaheen Afridi           PAK         630\n",
      "10     Kuldeep Yadav           IND         622\n"
     ]
    }
   ],
   "source": [
    "# c) Top 10 ODI bowlers along with the records of their team andrating.\n",
    "\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml' )\n",
    "\n",
    "#get the first player data :\n",
    "first_player_name = soup.find( 'div' , {'class' : 'rankings-block__banner--name-large'} ).text\n",
    "first_player_nationality = soup.find( 'div' , {'class' : 'rankings-block__banner--nationality'} ).text.strip()\n",
    "first_player_rating = soup.find('div' , {'class' : 'rankings-block__banner--rating'} ).text\n",
    "pos1 = soup.find('span' , {'class' : 'rankings-block__pos-number'}).text.strip()\n",
    "\n",
    "rank=[]\n",
    "names=[]\n",
    "nationalities = []\n",
    "rate_points = []\n",
    "\n",
    "#get the other 9 players :\n",
    "player = soup.find_all('tr' , {'class' : \"table-body\"} )\n",
    "for i in range(0,9): # t get 9 players only\n",
    "    position = player[i].find('span' , {'class' : 'rankings-table__pos-number'}).text.strip()\n",
    "    name = player[i].find('td' ,{'class' : 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    nationality = player[i].find('span' ,{'class' : 'table-body__logo-text'}).text.strip()\n",
    "    rate = player[i].find('td' , {'class' , 'table-body__cell rating' }).text.strip()\n",
    "    \n",
    "    rank.append(position)\n",
    "    names.append(name)\n",
    "    nationalities.append(nationality)\n",
    "    rate_points.append(rate)\n",
    "    \n",
    "\n",
    "    \n",
    "rank.insert(0 , pos1)    \n",
    "names.insert(0 , first_player_name)\n",
    "nationalities.insert(0, first_player_nationality)\n",
    "rate_points.insert(0 ,first_player_rating )\n",
    "\n",
    "print(rank)\n",
    "print(names)\n",
    "print(nationalities)\n",
    "print(rate_points)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index = rank)\n",
    "df['names']= names\n",
    "df['nationalities'] = nationalities\n",
    "df['rate points'] = rate_points\n",
    "\n",
    "print('\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb49281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e761e65",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170e03a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Teams Matches  points ratings\n",
      "1    AUS      26    4290     165\n",
      "2    ENG      31    3875     125\n",
      "3     SA      26    3098     119\n",
      "4    IND      30    3039     101\n",
      "5     NZ      28    2688      96\n",
      "6     WI      29    2743      95\n",
      "7    BAN      17    1284      76\n",
      "8     SL      12     820      68\n",
      "9    THA      13     883      68\n",
      "10   PAK      27    1678      62\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating\n",
    "\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml' )\n",
    "#find first rank\n",
    "team1 = soup.find('span' , {'class' , \"u-show-phablet\"}).text.strip()\n",
    "matches1 = soup.find('td' , {'class' , 'rankings-block__banner--matches'}).text.strip()\n",
    "points1 = soup.find('td' , {'class' , 'rankings-block__banner--points'}).text.strip()\n",
    "rating1 = soup.find('td' , {'class' , 'rankings-block__banner--rating u-text-right'}).text.strip()\n",
    "\n",
    "countries = []\n",
    "matchess = []\n",
    "ratingss = []\n",
    "#find other 9 teams :\n",
    "teams = soup.find_all('tr' , {'class' : \"table-body\"} )\n",
    "for i in range(0,9):  \n",
    "    country =  teams[i].find('span' , {'class' : 'u-show-phablet'}).text.strip()\n",
    "    matches =  teams[i].find('td' , {'class' : 'table-body__cell u-center-text'}).text\n",
    "    ratings =   teams[i].find('td' , {'class' : 'table-body__cell u-text-right rating'}).text\n",
    "    countries.append(country)\n",
    "    matchess.append(matches)\n",
    "    ratingss.append(ratings)\n",
    "    \n",
    "countries.insert(0 , team1)\n",
    "matchess.insert(0 , matches1)\n",
    "ratingss.insert(0 , rating1)\n",
    "points = [4290,3875,3098,3039,2688,2743,1284,820,883,1678]\n",
    "\n",
    "label = list(range(1,11))\n",
    "df = pd.DataFrame(index = label)\n",
    "df['Teams']= countries\n",
    "df['Matches']= matchess\n",
    "df['points']=points\n",
    "df['ratings'] = ratingss\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601526bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "['Natalie Sciver-Brunt', 'Chamari Athapaththu', 'Beth Mooney', 'Laura Wolvaardt', 'Smriti Mandhana', 'Alyssa Healy', 'Harmanpreet Kaur', 'Ellyse Perry', 'Meg Lanning', 'Stafanie Taylor']\n",
      "['ENG', 'SL', 'AUS', 'SA', 'IND', 'AUS', 'IND', 'AUS', 'AUS', 'WI']\n",
      "['803', '758', '751', '732', '708', '702', '694', '686', '682', '618']\n",
      "\n",
      "                    names nationalities rate points\n",
      "1   Natalie Sciver-Brunt           ENG         803\n",
      "2    Chamari Athapaththu            SL         758\n",
      "3            Beth Mooney           AUS         751\n",
      "4        Laura Wolvaardt            SA         732\n",
      "5        Smriti Mandhana           IND         708\n",
      "6           Alyssa Healy           AUS         702\n",
      "7       Harmanpreet Kaur           IND         694\n",
      "8           Ellyse Perry           AUS         686\n",
      "9            Meg Lanning           AUS         682\n",
      "10       Stafanie Taylor            WI         618\n"
     ]
    }
   ],
   "source": [
    "# b) Top 10 ODI Batswomen along with the records of their team andrating.\n",
    "\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml' )\n",
    "\n",
    "#get the first player data :\n",
    "first_player_name = soup.find( 'div' , {'class' : 'rankings-block__banner--name-large'} ).text\n",
    "first_player_nationality = soup.find( 'div' , {'class' : 'rankings-block__banner--nationality'} ).text.strip()\n",
    "first_player_rating = soup.find('div' , {'class' : 'rankings-block__banner--rating'} ).text\n",
    "pos1 = soup.find('span' , {'class' : 'rankings-block__pos-number'}).text.strip()\n",
    "\n",
    "rank=[]\n",
    "names=[]\n",
    "nationalities = []\n",
    "rate_points = []\n",
    "\n",
    "#get the other 9 players :\n",
    "player = soup.find_all('tr' , {'class' : \"table-body\"} )\n",
    "for i in range(0,9): # t get 9 players only\n",
    "    position = player[i].find('span' , {'class' : 'rankings-table__pos-number'}).text.strip()\n",
    "    name = player[i].find('td' ,{'class' : 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    nationality = player[i].find('span' ,{'class' : 'table-body__logo-text'}).text.strip()\n",
    "    rate = player[i].find('td' , {'class' , 'table-body__cell rating' }).text.strip()\n",
    "    \n",
    "    rank.append(position)\n",
    "    names.append(name)\n",
    "    nationalities.append(nationality)\n",
    "    rate_points.append(rate)\n",
    "    \n",
    "\n",
    "    \n",
    "rank.insert(0 , pos1)    \n",
    "names.insert(0 , first_player_name)\n",
    "nationalities.insert(0, first_player_nationality)\n",
    "rate_points.insert(0 ,first_player_rating )\n",
    "\n",
    "print(rank)\n",
    "print(names)\n",
    "print(nationalities)\n",
    "print(rate_points)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index = rank)\n",
    "df['names']= names\n",
    "df['nationalities'] = nationalities\n",
    "df['rate points'] = rate_points\n",
    "\n",
    "print('\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a4a1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "['Sophie Ecclestone', 'Shabnim Ismail', 'Jess Jonassen', 'Ashleigh Gardner', 'Megan Schutt', 'Hayley Matthews', 'Kate Cross', 'Ayabonga Khaka', 'Deepti Sharma', 'Rajeshwari Gayakwad']\n",
      "['ENG', 'SA', 'AUS', 'AUS', 'AUS', 'WI', 'ENG', 'SA', 'IND', 'IND']\n",
      "['769', '722', '682', '673', '666', '662', '661', '634', '607', '599']\n",
      "\n",
      "                   names nationalities rate points\n",
      "1     Sophie Ecclestone           ENG         769\n",
      "2        Shabnim Ismail            SA         722\n",
      "3         Jess Jonassen           AUS         682\n",
      "4      Ashleigh Gardner           AUS         673\n",
      "5          Megan Schutt           AUS         666\n",
      "6       Hayley Matthews            WI         662\n",
      "7            Kate Cross           ENG         661\n",
      "8        Ayabonga Khaka            SA         634\n",
      "9         Deepti Sharma           IND         607\n",
      "10  Rajeshwari Gayakwad           IND         599\n"
     ]
    }
   ],
   "source": [
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "# b) Top 10 ODI Batswomen along with the records of their team andrating.\n",
    "\n",
    "page = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml' )\n",
    "\n",
    "#get the first player data :\n",
    "first_player_name = soup.find( 'div' , {'class' : 'rankings-block__banner--name-large'} ).text\n",
    "first_player_nationality = soup.find( 'div' , {'class' : 'rankings-block__banner--nationality'} ).text.strip()\n",
    "first_player_rating = soup.find('div' , {'class' : 'rankings-block__banner--rating'} ).text\n",
    "pos1 = soup.find('span' , {'class' : 'rankings-block__pos-number'}).text.strip()\n",
    "\n",
    "rank=[]\n",
    "names=[]\n",
    "nationalities = []\n",
    "rate_points = []\n",
    "\n",
    "#get the other 9 players :\n",
    "player = soup.find_all('tr' , {'class' : \"table-body\"} )\n",
    "for i in range(0,9): # t get 9 players only\n",
    "    position = player[i].find('span' , {'class' : 'rankings-table__pos-number'}).text.strip()\n",
    "    name = player[i].find('td' ,{'class' : 'table-body__cell rankings-table__name name'}).text.strip()\n",
    "    nationality = player[i].find('span' ,{'class' : 'table-body__logo-text'}).text.strip()\n",
    "    rate = player[i].find('td' , {'class' , 'table-body__cell rating' }).text.strip()\n",
    "    \n",
    "    rank.append(position)\n",
    "    names.append(name)\n",
    "    nationalities.append(nationality)\n",
    "    rate_points.append(rate)\n",
    "    \n",
    "\n",
    "    \n",
    "rank.insert(0 , pos1)    \n",
    "names.insert(0 , first_player_name)\n",
    "nationalities.insert(0, first_player_nationality)\n",
    "rate_points.insert(0 ,first_player_rating )\n",
    "\n",
    "print(rank)\n",
    "print(names)\n",
    "print(nationalities)\n",
    "print(rate_points)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index = rank)\n",
    "df['names']= names\n",
    "df['nationalities'] = nationalities\n",
    "df['rate points'] = rate_points\n",
    "\n",
    "print('\\n',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541ecec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "875318cd",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64404a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK headline inflation rate drops sharply to 6.8% in July, in line with expectations\n"
     ]
    }
   ],
   "source": [
    "#Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "\n",
    "#HeadLine\n",
    "\n",
    "page = requests.get('https://www.cnbc.com/world/?region=world')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml')\n",
    "headline = soup.find('div' , {'class': 'FeaturedCard-contentText'})\n",
    "headline_text = headline.find('h2').text.strip()\n",
    "print(headline_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7ff7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UK headline inflation rate drops sharply to 6.8% in July, in line with expectations\n",
      "CNBC Daily Open: More trouble ahead for U.S. banks\n",
      "World leaders at risk of ignoring fossil fuel time bomb, climate chief says\n",
      "European markets expected to open lower as investors assess UK inflation data\n",
      "Target will report earnings before the bell — here's what to expect\n",
      "JPMorgan hikes default forecast for emerging markets amid China contagion fears\n",
      "3 unexpected traits of bad bosses and the tell-tale signs of 'dysfunction'\n",
      "Southeast Asia turns to alternative meats as fight against climate change ramps up\n",
      "Morgan Stanley says these firms are already making money from generative A.I.\n",
      "HSBC names 9 global 'unloved stocks' that look cheap and could be about to surge\n",
      "Top Morningstar strategist says sell these 6 'overvalued' stocks, names 5 to buy\n",
      "Asia markets fall as bank stocks retreat under pressure on Wall Street \n",
      "CNBC Daily Open: More obstacles for U.S. banks\n",
      "Cramer's Lightning Round: Don't touch ChargePoint\n",
      "Norfolk Southern CEO on Ohio derailment: company says it's improving safety \n",
      "Cramer points out stocks to watch as China's economy slows down\n",
      "E-cigarette use linked to higher risk of respiratory problems among youth, study says\n",
      "Apple moves iPhone 'end call' button back to middle in latest beta\n",
      "Stock futures are little changed as Wall Street looks beyond losing session\n",
      "Jeffrey Epstein referred Obama WH counsel to JPMorgan as customer\n",
      "Trump stiffed his alleged co-conspirators, whose claims brought in $250 million \n",
      "Stocks making the biggest moves after the bell: H&R Block, Cava, Stride and more\n",
      "Hedge funds beefed up A.I. bets in the second quarter\n",
      "Disney accused of withholding hundreds of millions of dollars from film financier\n",
      "Mediterranean restaurant chain Cava swings to a profit in first report since IPO\n",
      "Home Depot results beat on the top and bottom lines. Here's what the pros say\n",
      "CFPB to crack down on data brokers, Chopra says at White House event \n",
      "This chart shows how exposed each of our 35 stocks is to China's faltering economy\n",
      "A.I. startup founder charged with defrauding investors, manipulating documents\n",
      "Mental coach: 5 ways to become a 'stellar performer' at work\n"
     ]
    }
   ],
   "source": [
    "#news link\n",
    "\n",
    "page = requests.get('https://www.cnbc.com/world/?region=world')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml')\n",
    "headline = soup.find_all('li' , {'class': 'LatestNews-item'})\n",
    "for i in range(len(headline)):\n",
    "    url = headline[i].find('a' , {'class' : 'LatestNews-headline' }).text\n",
    "    print(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8db63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cnbc.com/2023/08/16/uk-headline-inflation-rate-drops-sharply-to-6point8percent-in-july-in-line-with-expectations.html\n",
      "https://www.cnbc.com/2023/08/16/stock-markets-more-trouble-ahead-for-us-banks.html\n",
      "https://www.cnbc.com/2023/08/16/climate-chief-world-leaders-at-risk-of-ignoring-fossil-fuel-time-bomb.html\n",
      "https://www.cnbc.com/2023/08/16/european-markets-open-to-close-earnings-data-and-news.html\n",
      "https://www.cnbc.com/2023/08/16/target-tgt-earnings-q2-2023.html\n",
      "https://www.cnbc.com/2023/08/16/jpmorgan-hikes-em-default-forecast-as-country-garden-drives-china-contagion-fears-.html\n",
      "https://www.cnbc.com/2023/08/16/3-unexpected-traits-of-bad-bosses-and-tell-tale-signs-of-dysfunction.html\n",
      "https://www.cnbc.com/2023/08/16/southeast-asia-turns-to-alternative-meats-as-fight-against-climate-change-ramps-up.html\n",
      "/pro/\n",
      "/pro/\n",
      "/pro/\n",
      "https://www.cnbc.com/2023/08/16/asia-markets.html\n",
      "https://www.cnbc.com/2023/08/16/stock-markets-more-obstacles-for-us-banks.html\n",
      "https://www.cnbc.com/2023/08/15/cramers-lightning-round-dont-touch-chargepoint.html\n",
      "https://www.cnbc.com/2023/08/15/norfolk-southern-ceo-on-ohio-derailment.html\n",
      "https://www.cnbc.com/2023/08/15/cramer-points-out-stocks-to-watch-as-chinas-economy-slows-down.html\n",
      "https://www.cnbc.com/2023/08/15/e-cigarette-youth-respiratory-problems.html\n",
      "https://www.cnbc.com/2023/08/15/apple-moves-iphone-end-call-button-back-to-middle-in-latest-beta.html\n",
      "https://www.cnbc.com/2023/08/15/stock-market-today-live-updates.html\n",
      "https://www.cnbc.com/2023/08/15/jeffrey-epstein-referred-obama-white-house-counsel-to-jpmorgan.html\n",
      "https://www.cnbc.com/2023/08/15/trump-alleged-co-conspirators-never-got-paid-by-trump-team.html\n",
      "https://www.cnbc.com/2023/08/15/stocks-making-biggest-moves-after-the-bell-hrb-cava-lrn-and-more.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2023/08/15/disney-accused-of-withholding-profits-from-tsg-entertainment.html\n",
      "https://www.cnbc.com/2023/08/15/cava-group-cava-q2-2023-earnings.html\n",
      "/pro/\n",
      "https://www.cnbc.com/2023/08/15/cfpb-to-crack-down-on-data-brokers-chopra-says-at-white-house-event-.html\n",
      "/investingclub/\n",
      "https://www.cnbc.com/2023/08/15/tech-founder-pumped-revenue-to-defraud-venture-investors-prosecutors.html\n",
      "https://www.cnbc.com/2023/08/15/5-tips-for-becoming-a-top-competitor-from-coach-who-trained-tom-brady.html\n"
     ]
    }
   ],
   "source": [
    "page = requests.get('https://www.cnbc.com/world/?region=world')\n",
    "source = page.text\n",
    "soup = BeautifulSoup(source , 'html.parser')\n",
    "headline = soup.find_all('li' , {'class': 'LatestNews-item'})\n",
    "\n",
    "for i in range(len(headline)):\n",
    "    URL = headline[i].find('a').get('href')\n",
    "    print(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4eb490",
   "metadata": {},
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0b8038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence\n"
     ]
    }
   ],
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "\n",
    "#paper title\n",
    "page = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml')\n",
    "header = soup.find('header' , {'class' : 'sc-orwwe2-2 iquSkg' })\n",
    "title = header.find('p' , {'class' : 'sc-orwwe2-5 jwxdyR'}).text\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e66f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Authors\n",
      "1   David Silver, Satinder Singh, Doina Precup, Ri...\n",
      "2                                          Tim Miller\n",
      "3                                   Margaret A. Boden\n",
      "4   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...\n",
      "5                      Ilaria Tiddi, Stefan Schlobach\n",
      "6                      Henry Prakken, Giovanni Sartor\n",
      "7     Richard S. Sutton, Doina Precup, Satinder Singh\n",
      "8           Kjersti Aas, Martin Jullum, Anders Løland\n",
      "9                Wenhan Luo, Junliang Xing and 4 more\n",
      "10                      Saurabh Arora, Prashant Doshi\n",
      "11  Jasper van der Waa, Elisabeth Nieuwburg, Anita...\n",
      "12  Joe Collenette, Katie Atkinson, Trevor Bench-C...\n",
      "13   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz\n",
      "14  Oskar Wysocki, Jessica Katharine Davies and 5 ...\n",
      "15  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...\n",
      "16          Nolan Bard, Jakob N. Foerster and 13 more\n",
      "17                         Ron Kohavi, George H. John\n",
      "18      Séverin Lemaignan, Mathieu Warnier and 3 more\n",
      "19    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz\n",
      "20                             Luigia Carlucci Aiello\n",
      "21             Patrick Lin, Keith Abney, George Bekey\n",
      "22     W. Bradley Knox, Alessandro Allievi and 3 more\n",
      "23  Leslie Pack Kaelbling, Michael L. Littman, Ant...\n",
      "24             Markus Langer, Daniel Oster and 6 more\n"
     ]
    }
   ],
   "source": [
    "#Authors\n",
    "page = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml')\n",
    "header = soup.find_all('li' , {'class' : 'sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs' })\n",
    "\n",
    "authors=[]\n",
    "for i in range(len(header)):\n",
    "    author = header[i].find('span' , {'class' : 'sc-1w3fpd7-0 dnCnAO'}).text.strip()\n",
    "    authors.append(author)\n",
    "\n",
    "df = pd.DataFrame(index = list(range(1 , len(header)+1 )))\n",
    "df['Authors']= authors\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaea58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Published Date\n",
      "1     October 2021\n",
      "2    February 2019\n",
      "3      August 1998\n",
      "4    February 2015\n",
      "5     January 2022\n",
      "6     October 2015\n",
      "7      August 1999\n",
      "8   September 2021\n",
      "9       April 2021\n",
      "10     August 2021\n",
      "11   February 2021\n",
      "12      April 2023\n",
      "13   November 2021\n",
      "14      March 2023\n",
      "15        May 2021\n",
      "16      March 2020\n",
      "17   December 1997\n",
      "18       June 2017\n",
      "19       June 2021\n",
      "20       June 2016\n",
      "21      April 2011\n",
      "22      March 2023\n",
      "23        May 1998\n",
      "24       July 2021\n"
     ]
    }
   ],
   "source": [
    "# Published Date\n",
    "page = requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml')\n",
    "header = soup.find_all('li' , {'class' : 'sc-9zxyh7-1 sc-9zxyh7-2 kOEIEO hvoVxs' })\n",
    "\n",
    "dates=[]\n",
    "for i in range(len(header)):\n",
    "    time = header[i].find('span' , {'class' : 'sc-1thf9ly-2 dvggWt'}).text.strip()\n",
    "    dates.append(time)\n",
    "    \n",
    "df = pd.DataFrame(index = list(range(1 , len(header)+1 )))\n",
    "df['Published Date']= dates\n",
    "print(df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc3a03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#skip-to-content-anchor\n",
      "http://www.elsevier.com\n",
      "https://account.elsevier.com/auth\n",
      "https://elsevier.com/about\n",
      "https://www.elsevier.com/connect\n",
      "https://www.elsevier.com/about/careers\n",
      "https://elsevier.com/about\n",
      "https://www.elsevier.com/connect\n",
      "https://www.elsevier.com/about/careers\n",
      "https://www.elsevier.com/rd-solutions\n",
      "https://www.elsevier.com/clinical-solutions\n",
      "https://www.elsevier.com/research-platforms\n",
      "https://www.elsevier.com/research-intelligence\n",
      "https://www.elsevier.com/education\n",
      "https://www.elsevier.com/solutions\n",
      "https://www.elsevier.com/rd-solutions\n",
      "https://www.elsevier.com/clinical-solutions\n",
      "https://www.elsevier.com/research-platforms\n",
      "https://www.elsevier.com/research-intelligence\n",
      "https://www.elsevier.com/education\n",
      "https://www.elsevier.com/solutions\n",
      "https://www.elsevier.com/authors\n",
      "https://www.elsevier.com/editors\n",
      "https://www.elsevier.com/reviewers\n",
      "https://www.elsevier.com/librarians\n",
      "https://www.elsevier.com/strategic-partners\n",
      "https://www.elsevier.com/open-access\n",
      "https://www.elsevier.com/societies\n",
      "https://www.elsevier.com/authors\n",
      "https://www.elsevier.com/editors\n",
      "https://www.elsevier.com/reviewers\n",
      "https://www.elsevier.com/librarians\n",
      "https://www.elsevier.com/strategic-partners\n",
      "https://www.elsevier.com/open-access\n",
      "https://www.elsevier.com/societies\n",
      "https://www.elsevier.com/books-and-journals\n",
      "https://webshop.elsevier.com/?utm_source=ecom&utm_medium=top&utm_campaign=webshop\n",
      "https://www.elsevier.com/books-and-journals\n",
      "https://webshop.elsevier.com/?utm_source=ecom&utm_medium=top&utm_campaign=webshop\n",
      "https://global-checkout.elsevier.com\n",
      "https://account.elsevier.com/auth\n",
      "https://www.sciencedirect.com/science/journal/00043702\n",
      "https://www.editorialmanager.com/artint/default.aspx\n",
      "https://www.elsevier.com/\n",
      "https://www.elsevier.com/search-results?labels=journals\n",
      "/artificial-intelligence\n",
      "/artificial-intelligence/most-downloaded-articles\n",
      "https://www.sciencedirect.com/science/journal/00043702\n",
      "https://www.editorialmanager.com/artint/default.aspx\n",
      "https://www.sciencedirect.com/science/journal/00043702\n",
      "https://www.elsevier.com/journals/artificial-intelligence/0004-3702/guide-for-authors\n",
      "https://authors.elsevier.com/tracking/landingpage/selection.do\n",
      "https://www.elsevier.com/journals/artificial-intelligence/0004-3702/subscribe?subscriptiontype=institutional\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000862\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370218305988\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370298000551\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370214001386\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221001788\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370215000910\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370299000521\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000539\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301958\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000515\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370220301533\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370223000073\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221001065\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370222001795\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000102\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370219300116\n",
      "https://www.sciencedirect.com/science/article/pii/S000437029700043X\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300790\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000096\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370216300224\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370211000178\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370222001692\n",
      "https://www.sciencedirect.com/science/article/pii/S000437029800023X\n",
      "https://www.sciencedirect.com/science/article/pii/S0004370221000242\n",
      "https://www.sciencedirect.com/science/journal/00043702\n",
      "https://www.sciencedirect.com/user/alerts\n",
      "https://www.sciencedirect.com/user/register?utm_campaign=sd_recommender_ELSJLS&utm_channel=elseco&dgcid=sd_recommender_ELSJLS\n",
      "http://www.elsevier.com/authors/home\n",
      "https://www.editorialmanager.com/artint/default.aspx\n",
      "https://www.editorialmanager.com/artint/default.aspx\n",
      "https://researcheracademy.elsevier.com\n",
      "https://www.elsevier.com/about/policies/copyright/permissions\n",
      "https://webshop.elsevier.com\n",
      "https://service.elsevier.com/app/home/supporthub/publishing/#authors\n",
      "https://authors.elsevier.com/tracking/landingpage/selection.do\n",
      "https://www.elsevier.com/librarians\n",
      "https://www.elsevier.com/journals/artificial-intelligence/0004-3702/subscribe?subscriptiontype=institutional\n",
      "http://www.elsevier.com/editors\n",
      "http://www.elsevier.com/editors/perk\n",
      "https://service.elsevier.com/app/home/supporthub/publishing/#editors\n",
      "http://www.elsevier.com/reviewers\n",
      "https://www.elsevier.com/reviewers/becoming-a-reviewer-how-and-why#recognizing\n",
      "https://service.elsevier.com/app/home/supporthub/publishing/#reviewers\n",
      "https://www.elsevier.com\n",
      "//www.elsevier.com/legal/elsevier-website-terms-and-conditions\n",
      "//www.elsevier.com/legal/privacy-policy\n",
      "//www.elsevier.com/legal/cookienotice\n",
      "//www.elsevier.com/sitemap\n",
      "https://www.relx.com/\n"
     ]
    }
   ],
   "source": [
    "#paper URL\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "requests = requests.get(url)\n",
    "source = requests.text \n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    " \n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eae61f",
   "metadata": {},
   "source": [
    "# Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4381ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         hotel name  \\\n",
      "0                   Castle Barbeque   \n",
      "1                        Cafe Knosh   \n",
      "2                 Castle's Barbeque   \n",
      "3              The Barbeque Company   \n",
      "4                       India Grill   \n",
      "5                    Delhi Barbeque   \n",
      "6  The Monarch - Bar Be Que Village   \n",
      "7                The Barbeque Times   \n",
      "8                 Indian Grill Room   \n",
      "\n",
      "                                             cuisine rate  \n",
      "0                     Connaught Place, Central Delhi    4  \n",
      "1  The Leela Ambience Convention Hotel,Shahdara, ...  4.3  \n",
      "2             Pacific Mall,Tagore Garden, West Delhi  3.9  \n",
      "3                 Gardens Galleria,Sector 38A, Noida  3.9  \n",
      "4               Hilton Garden Inn,Saket, South Delhi  3.9  \n",
      "5     Taurus Sarovar Portico,Mahipalpur, South Delhi  3.7  \n",
      "6  Indirapuram Habitat Centre,Indirapuram, Ghaziabad  3.8  \n",
      "7              M2K Corporate Park,Sector 51, Gurgaon  4.1  \n",
      "8   Suncity Business Tower,Golf Course Road, Gurgaon  4.3  \n"
     ]
    }
   ],
   "source": [
    "page = requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "source = page.content\n",
    "soup = BeautifulSoup(source , 'lxml')\n",
    "hotel = soup.find_all('div' , {'class':'restnt-card restaurant'})\n",
    "\n",
    "names=[]\n",
    "cuisines=[]\n",
    "\n",
    "for i in range(len(hotel)):\n",
    "    name = hotel[i].find('a' , {'class' : 'restnt-name ellipsis'}).text\n",
    "    cuisine = hotel[i].find('div' , {'class' : 'restnt-loc ellipsis'}).text\n",
    "    \n",
    "    \n",
    "    names.append(name)\n",
    "    cuisines.append(cuisine)\n",
    "\n",
    "rates = []\n",
    "hotel_rate = soup.find_all('div' , {'class' : 'img-wrap'})\n",
    "for i in range(len(hotel_rate)):\n",
    "    hotel_rating = hotel_rate[i].find('div' , {'class' : 'restnt-rating rating-4'}).text.strip()\n",
    "    rates.append(hotel_rating)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['hotel name']= names\n",
    "df['cuisine'] = cuisines\n",
    "df['rate'] = rates\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eed7752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         hotel name  \\\n",
      "0                   Castle Barbeque   \n",
      "1                        Cafe Knosh   \n",
      "2                 Castle's Barbeque   \n",
      "3              The Barbeque Company   \n",
      "4                       India Grill   \n",
      "5                    Delhi Barbeque   \n",
      "6  The Monarch - Bar Be Que Village   \n",
      "7                The Barbeque Times   \n",
      "8                 Indian Grill Room   \n",
      "\n",
      "                                             cuisine rate      location  \n",
      "0                     Connaught Place, Central Delhi    4           NaN  \n",
      "1  The Leela Ambience Convention Hotel,Shahdara, ...  4.3    East Delhi  \n",
      "2             Pacific Mall,Tagore Garden, West Delhi  3.9    West Delhi  \n",
      "3                 Gardens Galleria,Sector 38A, Noida  3.9         Noida  \n",
      "4               Hilton Garden Inn,Saket, South Delhi  3.9   South Delhi  \n",
      "5     Taurus Sarovar Portico,Mahipalpur, South Delhi  3.7   South Delhi  \n",
      "6  Indirapuram Habitat Centre,Indirapuram, Ghaziabad  3.8     Ghaziabad  \n",
      "7              M2K Corporate Park,Sector 51, Gurgaon  4.1       Gurgaon  \n",
      "8   Suncity Business Tower,Golf Course Road, Gurgaon  4.3       Gurgaon  \n"
     ]
    }
   ],
   "source": [
    "df['location'] = df['cuisine'].str.split(',').str.get(2)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
